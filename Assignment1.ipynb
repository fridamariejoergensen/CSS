{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d09f115",
   "metadata": {},
   "source": [
    "Link to Github repository: https://github.com/fridamariejoergensen/CSS.git <br>\n",
    "### Contribution Statement\n",
    "\n",
    "We, Frida(s206182), Cecilie(sXXXXXX), and Marie(sXXXXXX), collaborated on Assignment 1 in Jupyter Notebook and made the following contributions:\n",
    "\n",
    "- Cecilie focused on this\n",
    "\n",
    "- Frida did this\n",
    "\n",
    "- Marie did this\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30382ee0",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "132c3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1b2ea",
   "metadata": {},
   "source": [
    "## Part 1: Using web-scraping to gather data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b23e40d1",
   "metadata": {},
   "source": [
    "### 2019 edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08faee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Poster\n",
    "LINK = \"https://2019.ic2s2.org/posters/\"\n",
    "r = requests.get(LINK) \n",
    "soup = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ea79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "listul = soup.find(\"div\",{\"class\":\"col-md-8 page-content-wrap\"})\n",
    "listli = listul.find_all(\"li\")\n",
    "\n",
    "allnames = []\n",
    "for i in range(len(listli)):\n",
    "    if listli[i].find(\"strong\"):\n",
    "        listli[i].strong.extract()\n",
    "    if listli[i].find(\"span\"):\n",
    "        listli[i].span.extract()\n",
    "    if listli[i].find(\"br\"):\n",
    "        listli[i].br.extract()\n",
    "        \n",
    "    names = listli[i].text.replace(\"\\n\",\"\").replace(\" and \",\",\").replace(\", \",\",\")\n",
    "    allnames = allnames + names.split(\",\")\n",
    "    \n",
    "\n",
    "\n",
    "un2019 = np.unique(np.array(allnames))\n",
    "len(un2019)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c9844ca",
   "metadata": {},
   "source": [
    "There was ?? unique researchers that made poster presentations in 2019  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec954c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Oral\n",
    "LINK = 'https://2019.ic2s2.org/oral-presentations/'\n",
    "r = requests.get(LINK)\n",
    "all_oral = BeautifulSoup(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bf750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the specific element of the site that contains the names\n",
    "paragraphs = all_oral.find_all(\"p\", {\"class\":\"\"})[3:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee708271",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "for p in paragraphs:\n",
    "    text = p.text.strip()\n",
    "    lines = text.split(' â€“ ')\n",
    "    names.extend([line.split('.')[0] for line in lines[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd52749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that extracts list elements that contain numbers.\n",
    "\n",
    "def remove_elements_with_numbers(names):\n",
    "    return [i for i in names if not any(j.isdigit() for j in i)]\n",
    "\n",
    "p = remove_elements_with_numbers(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba66ceac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m newlist \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m p \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m      2\u001b[0m newlist\u001b[39m.\u001b[39mremove(\u001b[39m'\u001b[39m\u001b[39mNo Presentation\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m final_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(newlist)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "newlist = [word for line in p for word in line.split(',')]\n",
    "newlist.remove('No Presentation')\n",
    "final_list = np.asarray(newlist)\n",
    "print(len(np.unique(final_list)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a598bb7",
   "metadata": {},
   "source": [
    "There was ?? unique researchers that made oral presentations in 2019  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dd86500",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Combine the two lists of poster and oral presentators\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m combined_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((final_list, un2019))\n\u001b[1;32m      3\u001b[0m \u001b[39m# Find the length of unique combined list that attended the conference in 2019 \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(combined_list)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Combine the two lists of poster and oral presentators\n",
    "combined_list = np.concatenate((final_list, un2019))\n",
    "# Find the length of unique combined list that attended the conference in 2019 \n",
    "print(len(np.unique(combined_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead125b",
   "metadata": {},
   "source": [
    "In total there was ?? unique researchers you got in 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5eca51",
   "metadata": {},
   "source": [
    "Explain one or two decisions you took during the web-scraping exercise, for 2019 or any other year. Why did you take this choice? How might your decision impact the final number of authors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e722e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d651449",
   "metadata": {},
   "source": [
    "## Part 2: Getting data from the Semantic Scholar API\n",
    "Consider the list of author ids you have found in Week 2, Part 3, first excercise. For each author, use the Academic Graph API to find: <br>\n",
    "- their aliases <br>\n",
    "- their name <br>\n",
    "- their papers <br>\n",
    "\n",
    "(**Share the number of authors you will use as starting point in this exercises. Add a comment clarifying how many IC2S2 editions you included and if the collaborators were included or not!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfab70c",
   "metadata": {},
   "source": [
    "Create three dataframe to store the data you have collected:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3e0f1",
   "metadata": {},
   "source": [
    "**Author dataset**: in the author dataset, one raw is one unique author, and each row contains the following information: <br>\n",
    "- authorId: (str) the id of the author <br>\n",
    "- name: (str) the name of the author <br> \n",
    "- aliases: (list) the aliases of the author <br>\n",
    "- citationCount: (int) the total number of citations received by an author <br>\n",
    "- field: (str) the s2FieldsOfStudy that occurs most times across an author's papers (you should first obtain the category for each s2FieldsOfStudy) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeb58765",
   "metadata": {},
   "source": [
    "**Paper dataset**: in the paper dataset, one row is one unique paper, and each row contains the following information: <br>\n",
    "- paperId: (str) the id of the paper <br>\n",
    "- title: (str) the title of the paper <br> \n",
    "- year: (int) the year of publication <br>\n",
    "- externalId.DOI: (str) the DOI of the paper <br>\n",
    "- citationCount: (int) the number of citations <br>\n",
    "- fields: (list) the fields included in the paper (you should first obtain the category for each s2FieldsOfStudy) <br>\n",
    "-  authorIds: (list) this is a list of author Ids, including all the authors of this paper that are in our author dataset\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e61da7",
   "metadata": {},
   "source": [
    "\n",
    "**Paper abstract dataset**: in the paper abstract dataset, one row is one unique paper, and each row contains the following information: <br>\n",
    "- paperId: (str) the id of the paper <br>\n",
    "- abstract: (str) the abstract of the paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up logging of events and errors:\n",
    "open('collectingData.log','a+')\n",
    "logging.basicConfig(filename='collectingData.log', \n",
    "filemode='a',\n",
    "level=logging.INFO, \n",
    "format='%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c034087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have collected 120065 names.\n"
     ]
    }
   ],
   "source": [
    "# Code to load pickle with all names of conference people and their co-authors:\n",
    "with open(\"pickles/all_names\",\"rb\") as fp:\n",
    "    all_names = pickle.load(fp)\n",
    "print(\"We have collected {} names.\".format(len(all_names.values())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38e92e87",
   "metadata": {},
   "source": [
    "The following is code that creates an object with the authorIds and initializes the whole setup of the dataframes etc.  Then the functions in the object can send requests to the Semantic Scholar API to build up the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f5f854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "\n",
    "author_url = BASE_URL + VERSION + \"author/batch\"\n",
    "\n",
    "class compSciData:\n",
    "    def __init__(self,authorIds,picklePath=\"pickles\"):\n",
    "        self.authorIds = authorIds\n",
    "        self.authordf = pd.DataFrame(columns = ['authorId','name', 'aliases','citationCount','field'])\n",
    "        self.paperdf = pd.DataFrame(columns = ['paperId','title', 'year','externalId.DOI','citationCount','fields','authorIds'])\n",
    "        self.abstractdf = pd.DataFrame(columns = ['paperId','abstract'])    \n",
    "        self.authordict= {}\n",
    "        self.paperdict = {}\n",
    "        self.abstractdict = {}\n",
    "        self.checkedAuthors = {}\n",
    "        self.faultyAuthors = {}\n",
    "        self.picklePath = picklePath\n",
    "        self.authorURL = BASE_URL + VERSION + \"author/batch\"\n",
    "        \n",
    "\n",
    "    def loadPickles(self):\n",
    "        if os.path.isfile(self.picklePath+\"/finalAuthorDf\"):\n",
    "            self.authordf = pickle.load( open(self.picklePath+\"/finalAuthorDf\", \"rb\" ) )\n",
    "            print(\"Opened pickle with author dataframe.\")\n",
    "            logging.info(\"Opened pickle with author dataframe.\")\n",
    "\n",
    "        if os.path.isfile(self.picklePath+\"/finalPaperDf\"):\n",
    "            self.paperdf = pickle.load( open( self.picklePath+\"/finalPaperDf\", \"rb\" ) )\n",
    "            print(\"Opened pickle with paper dataframe.\")\n",
    "            logging.info(\"Opened pickle with paper dataframe.\")\n",
    "\n",
    "        if os.path.isfile(self.picklePath+\"/finalAbstractDf\"):\n",
    "            self.abstractdf = pickle.load( open( self.picklePath+\"/finalPaperDf\", \"rb\" ) )\n",
    "            print(\"Opened pickle with abstract dataframe.\")\n",
    "            logging.info(\"Opened pickle with abstract dataframe.\")\n",
    "\n",
    "        if os.path.isfile(self.picklePath+\"/checkedAuthors\"):\n",
    "            self.checkedAuthors = pickle.load( open( self.picklePath+\"/checkedAuthors\", \"rb\" ) )\n",
    "            print(\"Opened pickle with checked authors of length {}.\".format(len(self.checkedAuthors)))\n",
    "            logging.info(\"Opened pickle with checked authors of length {}.\".format(len(self.checkedAuthors)))\n",
    "\n",
    "\n",
    "    def handleRequests(self,cleanAuthBatch):\n",
    "        #Send request for the given ids:\n",
    "        params = {\"ids\":cleanAuthBatch,\n",
    "                                \"limit\":10000}\n",
    "        r = requests.post(self.authorURL+\"?fields=aliases,name,papers,papers.title,papers.abstract,papers.citationCount,\"\n",
    "        +\"papers.s2FieldsOfStudy,papers.year,papers.externalIds,papers.authors\", json=params)\n",
    "        data = r.json()\n",
    "        \n",
    "        # If data returns message, there is some error, which is likely either too many requests or an internal server error\n",
    "        if type(data)==list:\n",
    "            self.sortAuthorDataInDataframesDict(data,cleanAuthBatch)\n",
    "            # Sleep three secs per request to not exceed 100 requests per 5 min.\n",
    "            time.sleep(3)\n",
    "        elif 'message' in data.keys():\n",
    "            print(\"Request returned a message: \"+data['message'])\n",
    "            logging.warning(\"Request returned a message: \"+data['message'])\n",
    "            if data['message']=='Internal server error' or data['message']=='Endpoint request timed out':\n",
    "                # Sleep three secs per request to not exceed 100 requests per 5 min.\n",
    "                time.sleep(3)\n",
    "                self.findFaultyData(cleanAuthBatch)\n",
    "            else:\n",
    "                #data['message']=='Something with too many requests':\n",
    "                time.sleep(30)\n",
    "                #To be written\n",
    "                pass\n",
    "\n",
    "    def findFaultyData(self, cleanAuthBatch):\n",
    "        batchLength = len(cleanAuthBatch)\n",
    "        print(\"Initiating search for faulty data with batchLength {}.\".format(batchLength))\n",
    "        logging.info(\"Initiating search for faulty data with batchLength {}.\".format(batchLength))\n",
    "        if batchLength>3:\n",
    "            middleindex = batchLength//2\n",
    "            halves = [cleanAuthBatch[:middleindex],cleanAuthBatch[middleindex:]]\n",
    "            for half in halves:\n",
    "                self.handleRequests(half)\n",
    "        elif batchLength>1:\n",
    "            halves = [[id] for id in cleanAuthBatch]\n",
    "            for half in halves:\n",
    "                self.handleRequests(half)\n",
    "        else:\n",
    "            print(\"Faulty id: Found that author with id {} returns a server error upon request.\".format(cleanAuthBatch))\n",
    "            logging.warning(\"Faulty id: Found that author with id {} returns a server error upon request.\".format(cleanAuthBatch))\n",
    "            #Save in the checked authors list, so we don't check author again.\n",
    "            self.checkedAuthors[cleanAuthBatch[0]]=1 \n",
    "            self.faultyAuthors[cleanAuthBatch[0]]=\"error\"          \n",
    "\n",
    "    \n",
    "    def sortAuthorDataInDataframesDict(self,data,cleanAuthBatch):\n",
    "        # Takes data object containing the info returned from a request with authorIds. Sorts the data\n",
    "        # into the dataframes contained in self.\n",
    "        print(\"Initiating sortAuthorDataInDataframesDict\")\n",
    "        logging.info(\"Initiating sortAuthorDataInDataframesDict\")\n",
    "\n",
    "        # Look at every author:\n",
    "        for i in range(len(data)):\n",
    "            # Only proceed if the request for current author did return something, aka didn't return None\n",
    "            if data[i]:\n",
    "                authorData = data[i]\n",
    "                papers = authorData['papers']\n",
    "                #initialize variables for later:\n",
    "                fields = []\n",
    "                citationCount = 0\n",
    "                \n",
    "                #Look at everyone of their papers:\n",
    "                for n in range(len(papers)):\n",
    "                    paper = papers[n]\n",
    "                    paper_fields = []\n",
    "                    for s2 in paper['s2FieldsOfStudy']:\n",
    "                        paper_fields.append(s2['category'])\n",
    "                    fields=fields+paper_fields\n",
    "                    citationCount += paper['citationCount']\n",
    "\n",
    "                    paper_authors = []\n",
    "                    for auth in paper['authors']:\n",
    "                        paper_authors.append(auth['authorId'])\n",
    "\n",
    "\n",
    "                    self.paperdict[paper['paperId']] = {'paperId':paper['paperId'],'title':paper['title'], 'year':paper['year'],\n",
    "                    'externalId.DOI': paper['externalIds'].get('DOI'),'citationCount':paper['citationCount'],'fields':[paper_fields],\n",
    "                    'authorIds':[paper_authors]}\n",
    "\n",
    "                    self.abstractdict[paper['paperId']] = {'paperId':paper['paperId'],'abstract':paper['abstract']}\n",
    "\n",
    "                self.authordict[authorData['authorId']] = {'authorId' : authorData['authorId'], 'name' : authorData['name'], \n",
    "                'aliases' : [authorData['aliases']],'citationCount':citationCount,'field': max(fields,key=fields.count) if fields else None}\n",
    "\n",
    "                # Add authorId to dict with checked authors:\n",
    "                self.checkedAuthors[authorData['authorId']]=1\n",
    "\n",
    "            else:\n",
    "                print(\"Author with id {} returned a Nonetype object.\".format(cleanAuthBatch[i]))\n",
    "                logging.info(\"Author with id {} returned a Nonetype object.\".format(cleanAuthBatch[i]))\n",
    "                self.checkedAuthors[cleanAuthBatch[i]]=1\n",
    "                self.faultyAuthors[cleanAuthBatch[i]]=\"none\"\n",
    "\n",
    "\n",
    "    def findData(self, batchsize=100):\n",
    "        # Avoid batchsize errors:\n",
    "        if batchsize>100:\n",
    "            print(\"Batchsize can maximum be 100 because we are requesting fields=papers attributes.\")\n",
    "            logging.error(\"Batchsize can maximum be 100 because we are requesting fields=papers attributes.\")\n",
    "            return\n",
    "        \n",
    "        # Find papers for every name:\n",
    "        batches = math.floor(len(self.authorIds)/batchsize)\n",
    "        for k in range(batches+1):\n",
    "            #Calculate indexes for next batch:\n",
    "            lowerbound = k*batchsize\n",
    "            if k == batches:\n",
    "                upperbound = lowerbound+len(self.authorIds)%batchsize\n",
    "            else:\n",
    "                upperbound = lowerbound+batchsize\n",
    "            \n",
    "            #Remove all authors that have already been checked:\n",
    "            authBatch = self.authorIds[lowerbound:upperbound]\n",
    "            cleanAuthBatch = []\n",
    "            for id in authBatch:\n",
    "                if id not in self.checkedAuthors:\n",
    "                    cleanAuthBatch.append(id)\n",
    "\n",
    "            #Only proceed if there are still authors left after cleaning\n",
    "            if len(cleanAuthBatch)>0:\n",
    "                \n",
    "                # Takes authorIds that have not yet been used and handles the request, errorhandling and puts data in \n",
    "                # to dataframes in helper functions:\n",
    "                self.handleRequests(cleanAuthBatch)\n",
    "\n",
    "                print(\"Code has processed {} names.\".format(upperbound))\n",
    "                logging.info(\"Code has processed {} names.\".format(upperbound))\n",
    "\n",
    "                #Save pickles during process as a back up.\n",
    "                if upperbound%5000==0:\n",
    "                    self.savePickles(path=\"pickles/{}\".format(upperbound))\n",
    "\n",
    "    \n",
    "    def savePickles(self, path=\"pickles\"):\n",
    "        #Concatenate dataframe so far with the recently collected dicts:\n",
    "        self.authordf = pd.concat([self.authordf,pd.DataFrame.from_dict(self.authordict,\"index\")], ignore_index=True)\n",
    "        self.paperdf = pd.concat([self.paperdf,pd.DataFrame.from_dict(self.paperdict,\"index\")], ignore_index=True)\n",
    "        self.abstractdf = pd.concat([self.abstractdf,pd.DataFrame.from_dict(self.abstractdict,\"index\")], ignore_index=True)\n",
    "\n",
    "        #Clear dicts, so we don't save them into the dataframe twice:\n",
    "        self.authordict={}\n",
    "        self.paperdict={}\n",
    "        self.abstractdict={}\n",
    "\n",
    "        #Create folder if it does not exist:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        # Save the pickles:\n",
    "        pickle.dump(self.authordf,open(\"{}/finalAuthorDf\".format(path),\"wb\"))\n",
    "        pickle.dump(self.paperdf,open(\"{}/finalPaperDf\".format(path),\"wb\"))\n",
    "        pickle.dump(self.abstractdf,open(\"{}/finalAbstractDf\".format(path),\"wb\"))\n",
    "        pickle.dump(self.checkedAuthors,open(\"{}/checkedAuthors\".format(path),\"wb\"))\n",
    "        print(\"Pickles saved succesfully to path {}!\".format(path))\n",
    "        logging.info(\"Pickles saved succesfully to path {}!\".format(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c227f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes approx 46 min for 6000 ids\n",
    "data = compSciData(authorIds=list(all_names.keys())[:10000],picklePath =\"pickles\")\n",
    "data.loadPickles()\n",
    "data.findData(batchsize=100)\n",
    "data.savePickles()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6727ed",
   "metadata": {},
   "source": [
    "How long is your final Author dataframe? How long is your final Paper dataframe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b07ebe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a4f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f400466",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The author dataframe has length:\",len(data.authordf))\n",
    "print(\"The paper dataframe has length:\",len(data.paperdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbadc5b6",
   "metadata": {},
   "source": [
    "## Part 3: Law of large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41229f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5d14bf7640b47197a6bbe8a44938f4fb26dc5f9096bfdaeef805483a9a318f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
